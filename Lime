import xgboost as xgb
import pandas as pd
import lime
import lime.lime_tabular

# Assuming you have trained your XGBoost model and loaded your dataset 'data' and 'target'
# Train your XGBoost model
xgb_model = xgb.XGBClassifier()
xgb_model.fit(data, target)

# Create the LIME explainer
explainer = lime.lime_tabular.LimeTabularExplainer(data, feature_names=[f'feature_{i}' for i in range(data.shape[1])],
                                                  class_names=[f'class_{i}' for i in range(len(xgb_model.classes_))],
                                                  mode='classification')

# Create a list to store explanations for each class
explanations = []
for class_idx in range(len(xgb_model.classes_)):
    class_explanations = []
    for _ in range(100):  # Sample 100 data points for each class
        # Sample a data point from the dataset (you can use other sampling methods)
        sample_idx = np.random.randint(data.shape[0])
        sample_data = data[sample_idx]

        # Explain the prediction for the sampled data point using LIME
        exp = explainer.explain_instance(sample_data, xgb_model.predict_proba, labels=[class_idx], num_features=5)
        class_explanations.append(exp.as_list(label=class_idx))

    # Store the explanations for the current class
    explanations.append(class_explanations)

# Print the decision paths for each class
for class_idx, class_name in enumerate(xgb_model.classes_):
    print(f"{class_name}")
    for exp in explanations[class_idx]:
        for feature, value in exp:
            if value > 0:
                print(f"{feature} > {value}")
            else:
                print(f"{feature} <= {-value}")
    print()
